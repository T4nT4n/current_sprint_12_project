{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23220ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported liraries for this file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe174fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "original_car_file = pd.read_csv('/Users/tanner/Downloads/car_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec8982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First glimpse of the data's general info\n",
    "print(original_car_file.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d402bd6",
   "metadata": {},
   "source": [
    "We see that there are some potential features that need to be modified. \n",
    "\n",
    "The features that contain null values:\n",
    "- VehicleType\n",
    "- Gearbox\n",
    "- Model\n",
    "- FuelType\n",
    "- NotRepaired\n",
    "\n",
    "The features with potentially unusual data types for the information that they contain:\n",
    "- DateCrawled --> Change to datetime\n",
    "- DateCreated --> Change to datetime\n",
    "- LastSeen --> Change to datetime\n",
    "\n",
    "It is certainly possible that there are other features types that need to be changed during encoding, but these are the most obvious types that need to be changed. In addition, most--if not all-- features will be individually inspected to ensure the feature is cleaned for upcoming model usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d74289",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1cc9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function prints general information for arbitrary series and dataframe\"\"\"\n",
    "\n",
    "def feature_summary(object):\n",
    "\n",
    "    #If the object is a series\n",
    "    if isinstance(object, pd.Series):\n",
    "        #First five entires of the object\n",
    "        print(\"First five entries:\")\n",
    "        print(object.head(5))\n",
    "        print(\"\")\n",
    "\n",
    "        #Statistical information\n",
    "        print(\"General statistical info:\")\n",
    "        print(object.describe())\n",
    "        print(\"\\n\")\n",
    "\n",
    "        #General information\n",
    "        print(\"General info:\")\n",
    "        print(object.info())\n",
    "        print(\"\\n\")\n",
    "\n",
    "        #Number of unique values\n",
    "        unique_values = object.nunique(dropna= False)\n",
    "        print(f'Number of unique values: {unique_values}')\n",
    "        print(\"\\n\")\n",
    "\n",
    "        #Null values\n",
    "        null_values = object.isnull().sum()\n",
    "        percent_null = ((null_values) / (len(object))) * 100 \n",
    "        print(f'Number of null values: {null_values} \\n')\n",
    "        print(f'Percetage of Null Values: {percent_null:.2f} %')\n",
    "        print(\"\\n\")\n",
    "        \n",
    "\n",
    "    #If the object is a dataframe\n",
    "    elif isinstance(object, pd.DataFrame):\n",
    "        #For loop runs through all columns\n",
    "        for col in object.columns:\n",
    "            #First five entries\n",
    "            print(\"First five entries\")\n",
    "            print(object[col].head(5))\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            #General statistical info\n",
    "            print(f'\\n=== Column: {col} ===')\n",
    "            print('General statistic info:')\n",
    "            print(object[col].describe())\n",
    "            print(\"\\n\")\n",
    "\n",
    "            #General information\n",
    "            print(\"General info:\")\n",
    "            print(object.info())\n",
    "            print(\"\\n\")\n",
    "\n",
    "            #Number of unique values\n",
    "            unique_values = object[col].nunique(dropna = False)\n",
    "            print(f'Number of unique values: {unique_values}')\n",
    "            print\n",
    "\n",
    "            #Null values\n",
    "            null_values = object[col].isnull().sum()\n",
    "            percent_null = ((null_values) / (len(object[col]))) * 100\n",
    "            print(f'Number of null values: {null_values} \\n')\n",
    "            print(f'Percentage of Null Values: {percent_null:.2f} %')\n",
    "\n",
    "    #If the object is neither a series nor a dataframe\n",
    "    else:\n",
    "        print(\"The input must be a pandas Series or DataFrame.\")\n",
    "\n",
    "\n",
    "\"\"\"This function helpes to impute medians with the Power Column in the Data PreProcessing section\"\"\"\n",
    "\n",
    "def power_median_imputation(dataframe, medians):\n",
    "    #Here we merge the medians into the dataframe\n",
    "    dataframe = dataframe.merge(medians, on=['Brand', 'Model'], how='left')\n",
    "\n",
    "    #Now we fill the missing Power_Imputed values using the brand_model_medians medians\n",
    "    dataframe['Power_Imputed'] = dataframe['Power_Imputed'].fillna(dataframe['MedianPower'])\n",
    "\n",
    "    #The helper column can be dropped\n",
    "    dataframe = dataframe.drop(columns=['MedianPower'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3c9be",
   "metadata": {},
   "source": [
    "# Minor Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d6036",
   "metadata": {},
   "source": [
    "Before we edit the original CSV file, we will create a copy of it to begin editing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79351023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a copy of 'original_car_file' so that we can make edits to it\n",
    "minor_preprocessed_car_file = original_car_file.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae68466",
   "metadata": {},
   "source": [
    "## DateCrawled Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c6592a",
   "metadata": {},
   "source": [
    "Here are the first few entries of the *DateCrawled* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First few entries of the DateCrawled Column\n",
    "print(minor_preprocessed_car_file['DateCrawled'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab3b198",
   "metadata": {},
   "source": [
    "Let's convert this feature to datetime type. This way we can easily information from the dates if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3df608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting DateCrawled to datetime type\n",
    "minor_preprocessed_car_file['DateCrawled'] = pd.to_datetime(minor_preprocessed_car_file['DateCrawled'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "#Ensuring the change worked\n",
    "print(minor_preprocessed_car_file['DateCrawled'].info())\n",
    "print(minor_preprocessed_car_file['DateCrawled'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a99a8",
   "metadata": {},
   "source": [
    "Since there are no null values in this feature, we can move onto the next feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04cacce",
   "metadata": {},
   "source": [
    "## DateCreated Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991cb9d",
   "metadata": {},
   "source": [
    "Here are the first entries of the DateCreated feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 10 entries of the DateCreated feature\n",
    "print(minor_preprocessed_car_file['DateCreated'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e0039",
   "metadata": {},
   "source": [
    "We see that all of the hours and minutes corresponding to each car is set to zero. This is likely something to be corrected later on. For now, we will convert this column to datetime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fe238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting DateCreated to datetime type\n",
    "minor_preprocessed_car_file['DateCreated'] = pd.to_datetime(minor_preprocessed_car_file['DateCreated'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "#Ensuring the change was made\n",
    "print(minor_preprocessed_car_file['DateCreated'].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d4dada",
   "metadata": {},
   "source": [
    "## LastSeen Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b41b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First five entries of the LastSeen feature\n",
    "print(minor_preprocessed_car_file['LastSeen'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937712d",
   "metadata": {},
   "source": [
    "Similar to the DateCreated and DateCrawled features, we need to change this to datetime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67176593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting LastSeen to datetime type\n",
    "minor_preprocessed_car_file['LastSeen'] = pd.to_datetime(minor_preprocessed_car_file['LastSeen'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "#Ensuring the changes were made\n",
    "print(minor_preprocessed_car_file['LastSeen'].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d141dc",
   "metadata": {},
   "source": [
    "# Initial Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca0c10",
   "metadata": {},
   "source": [
    "In this section we perform basic analysis on the features that we may use. Ultimately, we should acquire a better understanding of the data and what we need to format within it for better model performance, readability, et cetera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86093255",
   "metadata": {},
   "source": [
    "## Analysis on DateCrawled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aec08b",
   "metadata": {},
   "source": [
    "Here is some basic information on the DateCrawled feature as well as the first few entries of the column. Since each timestamp is unique with respect to its hours and minutes, we are going to floor the hour and minutes to zero so that we solely get the date to sort data by, but still maintaining the datetime type which is useful if we want to use datetime methods in future analysis or preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9fd5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove the hours and minutes in a copy of the DateCrawled column. This enables us to have better general info below.\n",
    "#First, let's us copy the DateCrawled column\n",
    "datecrawled_copy = minor_preprocessed_car_file['DateCrawled'].copy(deep=True)\n",
    "\n",
    "#Now we floor the hours and minutes\n",
    "datecrawled_copy = datecrawled_copy.dt.floor('D')\n",
    "\n",
    "#Information about DateCrawled\n",
    "feature_summary(datecrawled_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7d3f5",
   "metadata": {},
   "source": [
    "The range of dates is March 5th to April 7th, a fairly short range. In addition, we have 34 unique value with no null entries. Let's visualize the distribution of this column by sorting the dates from greatest to lowest frequency. This will enable us to see which dates had greatest number of car profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure size and figure type\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "#We remove the hour and miute specifier and list the frequency with each corresponding date\n",
    "daily_counts = minor_preprocessed_car_file['DateCrawled'].dt.date.value_counts().sort_index()\n",
    "\n",
    "#Figure type\n",
    "sns.lineplot(x = daily_counts.index, y = daily_counts.values, color='steelblue', marker='o')\n",
    "\n",
    "#Figure's Title\n",
    "plt.title('Car Profiles Downloaded from March to April', fontsize = 14)\n",
    "\n",
    "#Figure's x and y axis' labels\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Number of Downloads')\n",
    "\n",
    "#Figure's Grid\n",
    "plt.grid(True, linestyle = '--', alpha = 0.5)\n",
    "\n",
    "#Results\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580bf0dd",
   "metadata": {},
   "source": [
    "The car profiles have fairly constant download rates, though there are certain dips throughout the month.  Since the downloads are relatively uniform, analyzing our data should be easier since each day has similar car profile totals associated with them. In fact, the hours and minutes will likely be irrelevant to use in our model training so we are going to rid of the hours and minute by flooring them down. Since there are no peculiar recordings in this feature, we can move onto the next feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9818286f",
   "metadata": {},
   "source": [
    "## Analysis on VehicleType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf40aa6",
   "metadata": {},
   "source": [
    "Here we perform analysis on 'VehicleType'. VehicleType simply describes the type of vehicle in the car profile such as a sedan or an SUV. Here is basic information on this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ac6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information about VehicleType\n",
    "feature_summary(minor_preprocessed_car_file['VehicleType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190ba3c",
   "metadata": {},
   "source": [
    "This feature has 8 unique values so we will check the distribution to see if the distribution is varied of if there is high imbalance. Also in this feature we have 37490 null values, which will need to be addressed in the data preprocessing. Let's visualize the distribution of this feature now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d47bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing NaN values with a label\n",
    "copy_df = minor_preprocessed_car_file.copy(deep = True)\n",
    "copy_df['VehicleType'] = copy_df['VehicleType'].fillna('Missing')\n",
    "\n",
    "#Creating the plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(data = copy_df, x = 'VehicleType', order = copy_df['VehicleType'].value_counts().index, palette='pastel', hue = 'VehicleType', legend = False)\n",
    "\n",
    "#Plot Labels\n",
    "plt.title(\"Number of Vehicle Types\", fontsize = 14)\n",
    "plt.xlabel(\"Vehicle Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "#Plot Formatting\n",
    "plt.xticks(rotation = 45)\n",
    "plt.grid(axis = 'y', linestyle='--', alpha = 0.5)\n",
    "\n",
    "#Results\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f8522",
   "metadata": {},
   "source": [
    "Many of the listings are concentrated into a few vehicle types, while other vehicle types appear less frequently. Knowing the data was sorted from greatest to least in terms of count, this distribution is clearly right-skewed with a long tail. In addition, the 'Missing' category is the 4th most populated category. Since it compromises much of the data, we will institute the null values as its own category in the dataframe as 'missing'. This will be performed in the Data Preprocessing section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc9206",
   "metadata": {},
   "source": [
    "## Analysis on RegistrationYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a83b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information about RegistrationYear\n",
    "feature_summary(minor_preprocessed_car_file['RegistrationYear'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74769f66",
   "metadata": {},
   "source": [
    "This feature has 151 unique values, none of them being null values. However, there are registration years that are clearly out of range for RegistrationYear. For example, the year 1000 does not make sense nor does the year 9999. Thus, we will need to filter them out when we are visualizing our distribution since they are clearly outliers. Specifically, will filter out any car older than 1950 and newer than 2020. Moreover, we will calculate how many rows were dropped in order to get distribution that we will plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40677d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of rows dropped after the filtering\n",
    "invalid_years = minor_preprocessed_car_file[(minor_preprocessed_car_file['RegistrationYear'] < 1950) | (minor_preprocessed_car_file['RegistrationYear'] > 2020)]\n",
    "row_removed_percentage = len(invalid_years) / len(minor_preprocessed_car_file['RegistrationYear'])\n",
    "\n",
    "#Result\n",
    "print(f'Invalid years removed: {len(invalid_years)}')\n",
    "print(f'Percent of rows removed: {row_removed_percentage * 100: .2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f06a5",
   "metadata": {},
   "source": [
    "Clearly the number of rows within these invalid years is considerably small compared to the overall size of the RegistrationYear feature. We may remove these rows completely in the Data Preprocessing section. Down below is the distribution of this feature with the filtered years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for valid years\n",
    "valid_years = minor_preprocessed_car_file[(minor_preprocessed_car_file['RegistrationYear'] >= 1950) & (minor_preprocessed_car_file['RegistrationYear'] <= 2020)]\n",
    "\n",
    "#Plot size\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "#Histogram setup\n",
    "sns.histplot(data=valid_years, x='RegistrationYear', bins = 100, palette='coolwarm', hue='RegistrationYear', legend=False)\n",
    "\n",
    "#Histogram formatting\n",
    "plt.title('Count of Registration Years', fontsize=14)\n",
    "plt.xlabel('Registration Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e852b5",
   "metadata": {},
   "source": [
    "This is a left-skewed, unimodal distribution whose greatest concentration of values lie in the years of 1999 to 2008, which correspond to the first and third quartiles respectively. The reasons for the cars contained within the tail (earlier / classic cars) are unknown; however, the used car market may have not been so established as it has been in recent years. Additionally, classic cars tend to be occupied in more niche markets since their pricing likely depends on different factors than those influencing modern, used cars. Moreover, regarding the dip in the newer, used cars is likely a consequence of more time needed before individuals want to sell the newer cars. Regardless, since 0.10% of rows are removed with this filtration, we will perform this change in the Data PreProcessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d578cb76",
   "metadata": {},
   "source": [
    "## Analysis on Gearbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfff228",
   "metadata": {},
   "source": [
    "This section is an analysis on the Gearbox feature. This represents the kind of Gearbox / transmission type for each sample in the dataframe. Down below is some general information about this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_summary(minor_preprocessed_car_file['Gearbox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7b021",
   "metadata": {},
   "source": [
    "There are two unique, valid entries in our Gearbox feature with 19833 null values, which is roughly 5.6% of the data. Since 5.6% is a decent portion of the data and imputation with the other two valid entires could cause misrepresentation of the data, creating a 'missing' category for the null values seem appropriate. It is possible that the null values could have some other meaning than what fits into 'auto' and 'manual'. Down below is a visualization of the distribution for this feature, which could provide deeper insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a817d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy of the dataframe\n",
    "gearbox_copy = minor_preprocessed_car_file.copy(deep=True)\n",
    "\n",
    "#Filling missing values with 'missing'\n",
    "gearbox_copy['Gearbox'] = gearbox_copy['Gearbox'].fillna('missing')\n",
    "\n",
    "#Figure size\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "#Figure type and setup\n",
    "sns.countplot(data=gearbox_copy, x='Gearbox', palette='pastel', hue='Gearbox', legend=False)\n",
    "\n",
    "#Figure add-ons\n",
    "plt.title(\"Gearbox Type Count\")\n",
    "plt.xlabel(\"Gearbox Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(axis='y', linestyle='--', alpha = 0.45)\n",
    "\n",
    "#Results\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a279126",
   "metadata": {},
   "source": [
    "We see that manual cars dominate this dataframe. Though there is certainly imbalance amongst these categories, since it is the target of our project, having imbalance is perfectly fine. Thus, in the Data PreProcessing section, we will map the null values to the 'missing' string and create its own category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15f3da",
   "metadata": {},
   "source": [
    "## Analysis of Power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee3b317",
   "metadata": {},
   "source": [
    "Here we perform analysis on the Power feature, which represents the horsepower (HP) of each car within our dataframe. Down below is general information about this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898923b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General information about the power column\n",
    "feature_summary(minor_preprocessed_car_file['Power'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11297761",
   "metadata": {},
   "source": [
    "We have 712 unique, valid entires in our Power feature with no invalid entries. However, based off the information above, we have values that are unrealistic in the context of the horsepower for used cars. The minimum value is 0 horsepower and the maximum value is 20000 horsepower. Clearly, these are outliers in our data so there will likely need to be some form of filtering in this feature. Looking at the first quartile, 69 horsepower, to the third quartile, 143 horsepower, is a realistic range of horsepowers for used, modern vehicles. Thus, this could be a reasonable filter to impose on this feature in the Preprocessing of this column. Additionally, the standard deviation is 189, which is extremely high. After the filtering, the standard deviation should lower considerably. We will visualize the distribution of the data to see if any deeper insight can be contrived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy of dataframe\n",
    "power_copy = minor_preprocessed_car_file['Power'].copy(deep=True)\n",
    "\n",
    "#Filter mask\n",
    "power_mask = power_copy <= 300\n",
    "\n",
    "#Applying mask\n",
    "filtered_copy = power_copy[power_mask]\n",
    "\n",
    "#Plot size\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "#Plot type and setup\n",
    "sns.histplot(x=filtered_copy, bins = 50, color='skyblue', edgecolor = 'black')\n",
    "\n",
    "#Plot add-ons\n",
    "plt.title(\"Distribution of Horse Power\")\n",
    "plt.xlabel('Horsepower')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis = 'y', linestyle='--', alpha = 0.45)\n",
    "\n",
    "#Output\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c7938",
   "metadata": {},
   "source": [
    "Clearly, zero horsepower is the dominate value in this distribution, which will need to be dealt with in the preprocessing. First, we filter out the zero, plot the distribution, and make inferences from it. Afterwards, the zeros can be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_power = minor_preprocessed_car_file[\n",
    "    (minor_preprocessed_car_file['Power'] > 0) &\n",
    "    (minor_preprocessed_car_file['Power'] < 400)\n",
    "]\n",
    "\n",
    "#Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(zero_power['Power'], bins = 40, color='orchid', edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Vehicle power distribution\")\n",
    "plt.xlabel('Horsepower')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98713cbc",
   "metadata": {},
   "source": [
    "Observing the HP range of  (0, 400], we acquire what one would expect. There is a flattening on the left side of the distribution, cars with extremely low HP values (not frequent). In the middle, between 100 and 150, we have the highest concentration of HP values, which are typical for modern cars. On the right side of the distribution, the sportier cars have higher HP values, but are also infrequent in the rental car industry. Overall, this distribution is reasonable. Below we calculate how much data is removed if we filter out any HP values above 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc2cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of cars removed\n",
    "removed_HP_rows = (minor_preprocessed_car_file['Power'] > 400).sum()\n",
    "\n",
    "#Size of dataset\n",
    "dataset_size = len(minor_preprocessed_car_file)\n",
    "\n",
    "#Percentage of rows removed\n",
    "row_removed_percentage = (removed_HP_rows / dataset_size) * 100\n",
    "\n",
    "#result\n",
    "print(f'The number of rows removed: {removed_HP_rows}\\n')\n",
    "print(f'Percent of data removed {row_removed_percentage} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43079ee",
   "metadata": {},
   "source": [
    "Removing any of the HP calues that are greater than 400 is roughly 0.20%, which is rather small. Thus, we will filter out HP values that are greater than 400 in the Data Preprocessing. However, the zero values still need to be accounted for. Down below is the percentage of the data that the zeros makes up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186108e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of zeros in dataset\n",
    "zero_HP_total = minor_preprocessed_car_file['Power'].value_counts(normalize=False, ascending=False)[0]\n",
    "\n",
    "#Relative frequency of zero in HP\n",
    "HP_relative_freq = minor_preprocessed_car_file['Power'].value_counts(normalize=True, ascending=False)[0]\n",
    "\n",
    "#Result\n",
    "print(f'Number of zero entries in dataset: {zero_HP_total}\\n')\n",
    "print(f'Percentage of zero values in dataset: {(HP_relative_freq * 100):.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552abe25",
   "metadata": {},
   "source": [
    "With a large percentage that zero HP take up, removing that many rows from our dataframe is not wise. What will be done in the Data PreProcessing is to compute the median of vehicles grouped by Brand and Model and then mapping each median with its corresponding zero entry. Instead of modifying the HP column directly, we will add another feature that captures these results. Moreover, during our model training, we can compare the results of when we perform this imputation and before imputation to see what yields better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f9bc5",
   "metadata": {},
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4fabe",
   "metadata": {},
   "source": [
    "From the Exploratory Data Analysis (EDA), we observe that the dataframe requires additional formatting. Before implementing these changes, we make a copy of the data that we were working with (minor_preprocessed_car_file) in the Initial Preprocessing and Initial EDA sections. This helps to seperate major changes to the dataframe throughout this project. The initial changes we perform in this section are ones that do not cause data leakage in our future validation / testing datasets. Thus, once the initial changes have been executed, then we split the preprocessed_df dataframe into training (60%), validation (20), and testing (20%) dataframes so that sensitive formatting can be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c4c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating copy of the initial preprocessed dataframe\n",
    "preprocessed_df = minor_preprocessed_car_file.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee767fe",
   "metadata": {},
   "source": [
    "## Preprocessing DateCrawled Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f764cfd",
   "metadata": {},
   "source": [
    "In the EDA section, it was stated that the hour and minute could be removed and give us a better handle on this feature. Thus, we are going to remove the hour and minutes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f478eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Removing' the hour and minute from our DateCrawled feature\n",
    "preprocessed_df['DateCrawled'] = pd.to_datetime(preprocessed_df['DateCrawled']).dt.floor('D')\n",
    "\n",
    "#Results\n",
    "print(preprocessed_df['DateCrawled'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3646b5b",
   "metadata": {},
   "source": [
    "## Preprocessing VehicleType Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e96ae",
   "metadata": {},
   "source": [
    "In the EDA of VehicleType, we found that we need to create a category for the null values since it comprised a majority of the data. This is executed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping null values to 'missing' in VehicleType feature\n",
    "preprocessed_df['VehicleType'] = preprocessed_df['VehicleType'].fillna('missing')\n",
    "\n",
    "#Results\n",
    "print(preprocessed_df['VehicleType'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ddf40",
   "metadata": {},
   "source": [
    "## Preprocessing RegistrationYear Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d604e4",
   "metadata": {},
   "source": [
    "We stated in the EDA that we need to filter the cars from 1950 to 2020. Thus, this is executed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering our dataframe to only include registration years from 1950 to 2020\n",
    "\n",
    "#Our mask which filters our preprocessed_df dataframe\n",
    "registration_year_mask = ((preprocessed_df['RegistrationYear'] >= 1950) & (preprocessed_df['RegistrationYear'] <= 2020))\n",
    "\n",
    "#Applying mask to the dataframe\n",
    "preprocessed_df = preprocessed_df[registration_year_mask]\n",
    "\n",
    "#Results\n",
    "print(\"The unique values of the Registration Year Feature:\")\n",
    "print(preprocessed_df['RegistrationYear'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818818c3",
   "metadata": {},
   "source": [
    "## Preprocessing Gearbox Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14cf7f1",
   "metadata": {},
   "source": [
    "In the EDA of the Gearbox feature, we observed that the null values comprised roughly 5% of the overall data. Moreover, dud to potential misrepresentation of the data and limited number of unique, valid gearbox values, creating a 'missing' category was the most appropriate action that can be taken for this feature so this change is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping null values to the 'missing' string to create a new category in this feature\n",
    "preprocessed_df['Gearbox'] = preprocessed_df['Gearbox'].fillna('missing')\n",
    "\n",
    "#Results\n",
    "print(preprocessed_df['Gearbox'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ba000",
   "metadata": {},
   "source": [
    "## Preprocessing Power Feature Before Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c9e9b",
   "metadata": {},
   "source": [
    "In the EDA, we stated two changes for the Power feature: First, we need to filter out the HP values that are greater than 400. Second, we need to compute the median of each car grouped by its Brand and Model, then map that median to any zero that is found within those specific car subsets. However, for the second change, we need to calculate these medians from the training data to prevent any data leakage when training our model. Thus, there will be two sections in Data Preprocessing dedicated to this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we only keep the rows with a HP values less than or equal to 400\n",
    "preprocessed_df = preprocessed_df[preprocessed_df['Power'] <= 400]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c989c6a",
   "metadata": {},
   "source": [
    "## Splitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab594539",
   "metadata": {},
   "source": [
    "Any changes beyond this point are dictated by the training portion of the dataset. This is to prevent any form of data leakage during model validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d928a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training / validation data (80%) and testing data (20%)\n",
    "train_valid_df, test_df = train_test_split(copy_df, test_size=0.2, random_state=12345)\n",
    "\n",
    "#80% of the training / validation will be split into training data (75%) and validation data (25%)\n",
    "train_df, valid_df = train_test_split(train_valid_df, test_size=0.25, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33f3d1",
   "metadata": {},
   "source": [
    "## Preprocessing Power Feature After Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768b3dc",
   "metadata": {},
   "source": [
    "As aforementioned in this section, we are going to replace all the zero values with the median of specific subsets of cars grouped by Brand and Model. This is performed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802752b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We add the 'Power_Imputed' column to our dataframes which is a copy of the Power column but the zeros are replaced with NaN values --> this will ensure our median values are calculated properly\n",
    "train_df['Power_Imputed'] = train_df['Power'].replace(0, pd.NA)\n",
    "valid_df['Power_Imputed'] = valid_df['Power'].replace(0, pd.NA)\n",
    "test_df['Power_Imputed'] = train_df['Power'].replace(0, pd.NA)\n",
    "\n",
    "\"\"\"\n",
    "Here we construct the multi-index (grouped by brand and model) series to calculate the medians of horsepower for each category, \n",
    "then renaming it to MedianPower since we are going to merge this with the dataframe that already has the Power_Imputed column name.\n",
    "\"\"\"\n",
    "brand_model_medians = train_df.groupby(['Brand', 'Model'])['Power_Imputed'].median().rename('MedianPower')\n",
    "\n",
    "#Now we are going to \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
